{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vanilla_Policy_Gradient .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abyaadrafid/Deep-Reinforcement-Learning/blob/master/Vanilla_Policy_Gradient_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# REINFORCE\n",
        "Reinforce is a monte carlo policy gradient method. It relies on estimated discounted returns using episode samples to update the parameters of the policy.\n",
        "We use a neural network to approximate the optimal policy of the agent. The policy of the agent is basically a probability distribution that describes the mapping of states to actions of the agent. \n",
        "\n",
        "Value based methods try to avoid learning the policy because it is often harder than value estimation. However, in some applications, approximating value functions is more complex that learning the optimal policy itself. This makes value iteration methods like Q learning less than ideal.\n",
        "\n",
        "REINFORCE takes gradients of the samples and uses them to update the policy iteratively. Rewards are collected from real trajectories. It relies on a full trajectory and that's why it is a Monte-Carlo method. It does not keep any memory of older trajectories. That is why we will not be using any replay buffers. We just need to store state trajectory for each episode until the episode is finished. \n",
        "\n",
        "We use the log probability of agent actions and discounted returns to update the policy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDgd50k07rII",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5dacceaa-b670-47f3-f901-876c39cec3d3"
      },
      "source": [
        "!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
        "!pip install stable-baselines[mpi]==2.10.0 box2d box2d-kengz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "zlib1g-dev set to manually installed.\n",
            "libopenmpi-dev is already the newest version (2.1.1-8).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 1s (831 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 144579 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting stable-baselines[mpi]==2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/fe/db8159d4d79109c6c8942abe77c7ba6b6e008c32ae55870a35e73fa10db3/stable_baselines-2.10.0-py3-none-any.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 4.4MB/s \n",
            "\u001b[?25hCollecting box2d\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/0b/d48d42dd9e19ce83a3fb4eee074e785b6c6ea612a2244dc2ef69427d338b/Box2D-2.3.10-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 14.5MB/s \n",
            "\u001b[?25hCollecting box2d-kengz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/20/51d6c0c87f7642efb709c518fb0ca8e5eab068259588552c41da5926ae27/Box2D-kengz-2.3.3.tar.gz (425kB)\n",
            "\u001b[K     |████████████████████████████████| 430kB 31.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.18.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.0.5)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.2)\n",
            "Collecting mpi4py; extra == \"mpi\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/8f/bbd8de5ba566dd77e408d8136e2bab7fdf2b97ce06cab830ba8b50a2f588/mpi4py-3.0.3.tar.gz (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 37.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2018.9)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.0.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->stable-baselines[mpi]==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Building wheels for collected packages: box2d-kengz, mpi4py\n",
            "  Building wheel for box2d-kengz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-kengz: filename=Box2D_kengz-2.3.3-cp36-cp36m-linux_x86_64.whl size=2000278 sha256=da3bfd48383d3ef154a44eb93ba93d0deb696dfc372a77301f18f5e50d980640\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/ae/e5/8bc678d262caad94659c199c540550e59d03dd3bd3684d4f1a\n",
            "  Building wheel for mpi4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.0.3-cp36-cp36m-linux_x86_64.whl size=2074478 sha256=5cfcbc46e2fcfa3f076071e65e4bc7342fc5a027b8bab3a482a5529fc18d4c68\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/e0/86/2b713dd512199096012ceca61429e12b960888de59818871d6\n",
            "Successfully built box2d-kengz mpi4py\n",
            "Installing collected packages: mpi4py, stable-baselines, box2d, box2d-kengz\n",
            "Successfully installed box2d-2.3.10 box2d-kengz-2.3.3 mpi4py-3.0.3 stable-baselines-2.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWEryLdLKcFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.distributions import Categorical\n",
        "%matplotlib inline\n",
        "\n",
        "plt.style.use('seaborn')"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvCDzsTYLEGU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c3b1e1d8-17da-48c9-85bf-145a8093ee00"
      },
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "env.seed(0)\n",
        "print(env.action_space)\n",
        "print(env.observation_space)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discrete(4)\n",
            "Box(8,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4Gd6nz3LGzN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYPGwSRcKT0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "  def __init__(self, state_size, fc1_size, fc2_size, action_size):\n",
        "    super(PolicyNetwork, self).__init__()\n",
        "    \n",
        "    self.layers = nn.Sequential(\n",
        "      nn.Linear(state_size, fc1_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(fc1_size, fc2_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(fc2_size, action_size),\n",
        "      nn.Softmax(dim = 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = self.layers(state)\n",
        "    dist = Categorical(x)\n",
        "\n",
        "    return dist"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrMz31kKLaGM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PolicyGradientAgent():\n",
        "  def __init__(self, lr, state_size, action_size, fc1_size = 128, fc2_size = 256,):\n",
        "    self.policy_network = PolicyNetwork(state_size, fc1_size, fc2_size, action_size).to(device)\n",
        "    self.optimizer = optim.Adam(self.policy_network.parameters(), lr)\n",
        "    self.action_memory = []\n",
        "    self.reward_memory = []\n",
        "\n",
        "  def choose_action(self, state):\n",
        "    action_probs = self.policy_network(state)\n",
        "    action = action_probs.sample()\n",
        "    log_probs = action_probs.log_prob(action)\n",
        "    self.action_memory.append(log_probs)\n",
        "\n",
        "    return action.item()\n",
        "\n",
        "  def store_rewards(self, reward):\n",
        "    self.reward_memory.append(reward)\n",
        "\n",
        "  def learn(self):\n",
        "    self.optimizer.zero_grad()\n",
        "    G = np.zeros_like(self.reward_memory, dtype= np.float64)\n",
        "    for t in range(len(self.reward_memory)):\n",
        "      G_sum = 0\n",
        "      discount = 1\n",
        "      for k in range(t, len(self.reward_memory)):\n",
        "        G_sum += self.reward_memory[k]*discount\n",
        "        discount *= gamma\n",
        "      G[t]= G_sum\n",
        "    \n",
        "    mean = np.mean(G)\n",
        "    std = np.std(G) if np.std(G)>0 else 1\n",
        "\n",
        "    G = (G-mean)/std\n",
        "    G = torch.tensor(G, dtype= torch.float).to(device)\n",
        "\n",
        "    loss = 0\n",
        "    for g, logprob in zip(G, self.action_memory):\n",
        "      loss += -g*logprob\n",
        "\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "    self.action_memory=[]\n",
        "    self.reward_memory=[]\n"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrRjGR7pnVbM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1e123c7f-ce80-4355-c0e4-aab570284d3d"
      },
      "source": [
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "print('State size: {}, action size: {}'.format(state_size, action_size))"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State size: 8, action size: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdLFQ1kzQahf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "policy_agent = PolicyGradientAgent(1e-3, state_size=state_size, action_size=action_size)\n",
        "n_episodes = 3000\n",
        "gamma = 0.99\n",
        "PRINT_EVERY = 50\n",
        "ENV_SOLVED = 200"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FoX4VeMm3JD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(agent):\n",
        "  all_score = []\n",
        "  score_history = []\n",
        "  score = 0\n",
        "  for i in range(n_episodes):\n",
        "    done = False\n",
        "    score = 0\n",
        "    state = env.reset()\n",
        "    while not done:\n",
        "      state = torch.Tensor(state).to(device)\n",
        "      action = agent.choose_action(state)\n",
        "      next_state, reward, done, info = env.step(action)\n",
        "      agent.store_rewards(reward)\n",
        "      state = next_state\n",
        "      score+= reward\n",
        "      \n",
        "    score_history.append(score)\n",
        "    agent.learn()\n",
        "    all_score.append(np.mean(score_history))\n",
        "\n",
        "    if i % PRINT_EVERY == 0 :\n",
        "      print('\\r Progress {}/{}, average score:{:.2f}'.format(i, n_episodes, np.mean(score_history)), end=\"\")\n",
        "    if score >= ENV_SOLVED:\n",
        "      print('\\rEnvironment solved in {} episodes, score: {:.2f}'.format(i, score), end=\"\")\n",
        "      sys.stdout.flush()\n",
        "      break\n",
        "  return all_score"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf2JYc3Fpt2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot(score, string):\n",
        "  plt.figure(figsize=(10,6))\n",
        "  plt.plot(score)\n",
        "  plt.plot(pd.Series(score).rolling(100).mean())\n",
        "  plt.title('%s Training,'%string)\n",
        "  plt.xlabel('# of episodes')\n",
        "  plt.ylabel('score')\n",
        "  plt.show()"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o5PLelSq58i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "42c2cd85-c9b5-434d-d3fc-88a8a652a610"
      },
      "source": [
        "scores = train(policy_agent)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Environment solved in 590 episodes, score: 251.65"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLIWdSAcTUIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}